{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfb9588",
   "metadata": {},
   "source": [
    "### Initialize Catalog\n",
    "\n",
    " - Connect to catalog (jdbc - postgres) and warehouse (s3 - data & metadata layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8f59cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connected to Iceberg catalog: `trinity`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trinity (<class 'pyiceberg.catalog.sql.SqlCatalog'>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from connection import connect_to_catalog\n",
    "catalog = connect_to_catalog()\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c724d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing namespaces: ['conformance']\n"
     ]
    }
   ],
   "source": [
    "name_spaces = [ns[0] for ns in catalog.list_namespaces()]\n",
    "print(f\"Existing namespaces: {name_spaces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec74fe5",
   "metadata": {},
   "source": [
    "### What did that do?\n",
    "\n",
    "Connecting to the catalog via the `connect_to_catalog` function:\n",
    "\n",
    "1. Establishes a connection with the jdbc catalog (i.e. postgres) and data/metadata warehouse (i.e. s3)\n",
    "\n",
    "2. With the connection established, the following tables are automatically added to the `public` schema if they do not exist:\n",
    "\n",
    "![](imgs/jdbc-cat.png)\n",
    "\n",
    "Note that we are using the postgres database from the container `pgduckdb/pgduckdb:17-main`. This image comes with the [pg_duckdb](https://github.com/duckdb/pg_duckdb) extension and schema pre-installed.\n",
    "\n",
    "---\n",
    "\n",
    "#### Now we can read in a schema to prepare for creating a table\n",
    "\n",
    "To do that we first need a table schema and optional partition specification. Then can set the target and create table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24b72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: table {\n",
      "  1: donor_model_id: required string (Identifier for the donor model)\n",
      "  2: donor_model_type: required string (Type of the donor model)\n",
      "  3: donor_model_version: required string (Version of the donor model)\n",
      "  4: donor_site_type: required string (Type of the donor site)\n",
      "  5: donor_site_id: required string (Identifier for the donor site)\n",
      "  6: receiver_model_id: required string (Identifier for the receiver model)\n",
      "  7: receiver_model_type: required string (Type of the receiver model)\n",
      "  8: receiver_model_version: required string (Version of the receiver model)\n",
      "  9: receiver_site_type: required string (Type of the receiver site)\n",
      "  10: receiver_site_id: required string (Identifier for the receiver site)\n",
      "}\n",
      "Identifier fields: {1: 'donor_model_id', 2: 'donor_model_type', 3: 'donor_model_version', 5: 'donor_site_id', 4: 'donor_site_type', 6: 'receiver_model_id', 7: 'receiver_model_type', 8: 'receiver_model_version', 10: 'receiver_site_id', 9: 'receiver_site_type'}\n"
     ]
    }
   ],
   "source": [
    "from table_utils import load_iceberg_schema_and_properties\n",
    "import json\n",
    "\n",
    "# Load / Preview schema\n",
    "models_schema_json = \"schemas/models.json\"\n",
    "\n",
    "MODEL_SCHEMA, MODEL_PROPS = load_iceberg_schema_and_properties(models_schema_json)\n",
    "\n",
    "print(f\"Schema: {MODEL_SCHEMA}\")\n",
    "\n",
    "# Identifier fields serve as primary keys\n",
    "print(\n",
    "    \"Identifier fields:\",\n",
    "    {fid: MODEL_SCHEMA.find_field(fid).name for fid in MODEL_SCHEMA.identifier_field_ids},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56280f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Spec: [\n",
      "  100: donor_model_id: identity(1)\n",
      "  101: donor_model_version: identity(3)\n",
      "  102: receiver_model_id: identity(6)\n",
      "  103: receiver_model_version: identity(8)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# This dataset will grow to many billions of rows, so partitioning is important.\n",
    "# Note: We can use predicate pushdown later on unpartitioned columns\n",
    "from table_utils import auto_partition_spec\n",
    "MODEL_PARTITION_SPEC = auto_partition_spec(MODEL_SCHEMA, [\"donor_model_id\", \"donor_model_version\", \"receiver_model_id\", \"receiver_model_version\"])\n",
    "print(f\"Partition Spec: {MODEL_PARTITION_SPEC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf019991",
   "metadata": {},
   "source": [
    "\n",
    "##### Now we can add a `namespace` for create an iceberg `table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a56a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Root: s3://trinity-pilot/warehouse\n",
      "Namespace already exists: conformance\n"
     ]
    }
   ],
   "source": [
    "# Set target: Load existing variables from iceberg/connection.py:\n",
    "# Create table namespace\n",
    "from connection import CATALOG_ROOT\n",
    "print(f\"Catalog Root: {CATALOG_ROOT}\")  \n",
    "\n",
    "# Assign table-specific variables\n",
    "TABLE_NAME_SPACE = \"conformance\"\n",
    "\n",
    "name_spaces = [ns[0] for ns in catalog.list_namespaces()]\n",
    "if TABLE_NAME_SPACE not in name_spaces:\n",
    "    catalog.create_namespace(TABLE_NAME_SPACE)\n",
    "    print(f\"Created namespace: {TABLE_NAME_SPACE}\")\n",
    "else:\n",
    "    print(f\"Namespace already exists: {TABLE_NAME_SPACE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d929327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trinity (<class 'pyiceberg.catalog.sql.SqlCatalog'>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b28e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "When initiating multiple part upload for key 'warehouse/conformance/models/metadata/00000-6f3092d4-e074-48da-a50b-6636215beb03.metadata.json' in bucket 'trinity-pilot': AWS Error ACCESS_DENIED during CreateMultipartUpload operation: User: arn:aws:iam::424342154607:user/kanawha-pilot-reader is not authorized to perform: s3:PutObject on resource: \"arn:aws:s3:::trinity-pilot/warehouse/conformance/models/metadata/00000-6f3092d4-e074-48da-a50b-6636215beb03.metadata.json\" because no identity-based policy allows the s3:PutObject action (Request ID: JAH58GVWB5AZW9AM)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTable `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPG_MODEL_TABLE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` already exists.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Create the Iceberg table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPG_MODEL_TABLE_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_SCHEMA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mS3_DATA_LOCATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_PROPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartition_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_PARTITION_SPEC\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTable `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPG_MODEL_TABLE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` has been created.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyiceberg/catalog/sql.py:221\u001b[39m, in \u001b[36mSqlCatalog.create_table\u001b[39m\u001b[34m(self, identifier, schema, location, partition_spec, sort_order, properties)\u001b[39m\n\u001b[32m    217\u001b[39m metadata = new_table_metadata(\n\u001b[32m    218\u001b[39m     location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n\u001b[32m    219\u001b[39m )\n\u001b[32m    220\u001b[39m io = load_file_io(properties=\u001b[38;5;28mself\u001b[39m.properties, location=metadata_location)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Session(\u001b[38;5;28mself\u001b[39m.engine) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyiceberg/catalog/__init__.py:1005\u001b[39m, in \u001b[36mMetastoreCatalog._write_metadata\u001b[39m\u001b[34m(metadata, io, metadata_path)\u001b[39m\n\u001b[32m   1003\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write_metadata\u001b[39m(metadata: TableMetadata, io: FileIO, metadata_path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     \u001b[43mToOutputFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyiceberg/serializers.py:130\u001b[39m, in \u001b[36mToOutputFile.table_metadata\u001b[39m\u001b[34m(metadata, output_file, overwrite)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtable_metadata\u001b[39m(metadata: TableMetadata, output_file: OutputFile, overwrite: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write a TableMetadata instance to an output file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m        output_file (OutputFile): A custom implementation of the iceberg.io.file.OutputFile abstract base class.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m        overwrite (bool): Where to overwrite the file if it already exists. Defaults to `False`.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43moutput_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m output_stream:\n\u001b[32m    131\u001b[39m         \u001b[38;5;66;03m# We need to serialize None values, in order to dump `None` current-snapshot-id as `-1`\u001b[39;00m\n\u001b[32m    132\u001b[39m         exclude_none = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m Config().get_bool(\u001b[33m\"\u001b[39m\u001b[33mlegacy-current-snapshot-id\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    134\u001b[39m         json_bytes = metadata.model_dump_json(exclude_none=exclude_none).encode(UTF8)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyiceberg/io/pyarrow.py:368\u001b[39m, in \u001b[36mPyArrowFile.create\u001b[39m\u001b[34m(self, overwrite)\u001b[39m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exists() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot create file, already exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.location\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     output_file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_output_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyarrow/_fs.pyx:913\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_output_stream\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lakehouse-venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: When initiating multiple part upload for key 'warehouse/conformance/models/metadata/00000-6f3092d4-e074-48da-a50b-6636215beb03.metadata.json' in bucket 'trinity-pilot': AWS Error ACCESS_DENIED during CreateMultipartUpload operation: User: arn:aws:iam::424342154607:user/kanawha-pilot-reader is not authorized to perform: s3:PutObject on resource: \"arn:aws:s3:::trinity-pilot/warehouse/conformance/models/metadata/00000-6f3092d4-e074-48da-a50b-6636215beb03.metadata.json\" because no identity-based policy allows the s3:PutObject action (Request ID: JAH58GVWB5AZW9AM)"
     ]
    }
   ],
   "source": [
    "MODEL_TABLE = \"models\"\n",
    "PG_MODEL_TABLE_NAME = f\"{TABLE_NAME_SPACE}.{MODEL_TABLE}\"\n",
    "S3_DATA_LOCATION = f\"{CATALOG_ROOT}/{TABLE_NAME_SPACE}/{MODEL_TABLE}\"\n",
    "\n",
    "if catalog.table_exists(PG_MODEL_TABLE_NAME):\n",
    "    print(f\"Table `{PG_MODEL_TABLE_NAME}` already exists.\")\n",
    "else:\n",
    "    # Create the Iceberg table\n",
    "    catalog.create_table(\n",
    "        identifier=PG_MODEL_TABLE_NAME,\n",
    "        schema=MODEL_SCHEMA,\n",
    "        location=S3_DATA_LOCATION,\n",
    "        properties=MODEL_PROPS,\n",
    "        partition_spec=MODEL_PARTITION_SPEC\n",
    "    )\n",
    "    print(f\"Table `{PG_MODEL_TABLE_NAME}` has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba33538",
   "metadata": {},
   "source": [
    "### Connect the newly created catalog to nimtable\n",
    "\n",
    "The Nimtable utility is a handy reference utility for viewing table data.\n",
    "\n",
    "Enter the information in the `Create Catalog` button available at http://localhost/3000\n",
    "\n",
    "![](imgs/new-nimtable-cat.png)\n",
    "\n",
    "Then view the table:\n",
    "\n",
    "![](imgs/cat-page-1.png)\n",
    "\n",
    "![](imgs/cat-page-2.png)\n",
    "\n",
    "![](imgs/cat-page-3.png)\n",
    "\n",
    "*NOTE* See schemas/hydrology for addtional properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be482228",
   "metadata": {},
   "source": [
    "#### Updates acheived:\n",
    "\n",
    "1. The postgres catalog no points to the newly create table in the conformance namespace:\n",
    "\n",
    "![](imgs/pg-iceberg-table-1.png)\n",
    "\n",
    "2. A newly created metadata file has been written to the s3 catalog + namespace location:\n",
    "\n",
    "![](imgs/bucket-view-1.png)\n",
    "\n",
    "*NOTE* There are no data files yet. \n",
    "\n",
    "---\n",
    "\n",
    "#### Finally, lets add a Hydraulics table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b81e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg Table Schema: table {\n",
      "  1: sim_time: required timestamp (Simulation timestamp from HEC-RAS model [UTC])\n",
      "  2: realization_id: required int (Unique identifier for each model realization)\n",
      "  3: model_id: required string (Identifier for the HEC-RAS model)\n",
      "  4: site_id: required string (Identifier for the measurement site)\n",
      "  5: event_id: required int (Identifier for the simulated event)\n",
      "  6: run_version: required string (Version of the model run)\n",
      "  7: flow: optional double (Discharge at the site [cfs])\n",
      "  8: stage: optional double (Stage at the site [ft])\n",
      "}\n",
      "Iceberg Table Properties: {\n",
      "  \"hydraulics.schema.version\": \"1.0.0\",\n",
      "  \"hydraulics.description\": \"HEC-RAS simulation outputs\",\n",
      "  \"hydraulics.units.convention\": \"English\",\n",
      "  \"hydraulic.model.version\": \"6.6\",\n",
      "  \"hydraulics.stac.catalog\": \"s3://trinity-pilot/stac/hydraulics/catalog.json\",\n",
      "  \"write.hive-style-partitioning\": \"true\",\n",
      "  \"write.target-file-size-bytes\": \"536870912\",\n",
      "  \"field.sim_time.unit\": \"UTC\",\n",
      "  \"field.sim_time.description\": \"Simulation timestamp from HEC-RAS model\",\n",
      "  \"field.realization_id.description\": \"Unique identifier for each model realization\",\n",
      "  \"field.model_id.description\": \"Identifier for the HEC-RAS model\",\n",
      "  \"field.site_id.description\": \"Identifier for the measurement site\",\n",
      "  \"field.event_id.description\": \"Identifier for the simulated event\",\n",
      "  \"field.run_version.description\": \"Version of the model run\",\n",
      "  \"field.flow.unit\": \"cfs\",\n",
      "  \"field.flow.description\": \"Discharge at the site\",\n",
      "  \"field.stage.unit\": \"ft\",\n",
      "  \"field.stage.description\": \"Stage at the site\"\n",
      "}\n",
      "Identifier fields: {1: 'sim_time', 2: 'realization_id', 3: 'model_id', 4: 'site_id', 5: 'event_id', 6: 'run_version'}\n",
      "Partition Spec: [\n",
      "  100: realization_id: identity(2)\n",
      "  101: model_id: identity(3)\n",
      "  102: run_version: identity(6)\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n",
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table `conformance.hydraulics` has been created.\n"
     ]
    }
   ],
   "source": [
    "#### Hydraulics Table\n",
    "\n",
    "# Load / Preview schema\n",
    "ras_schema_json = \"schemas/hydraulics.json\"\n",
    "\n",
    "HYDRAULICS_SCHEMA, HYDRAULICS_PROPS = load_iceberg_schema_and_properties(ras_schema_json)\n",
    "\n",
    "print(f\"Iceberg Table Schema: {HYDRAULICS_SCHEMA}\")\n",
    "print(f\"Iceberg Table Properties: {json.dumps(HYDRAULICS_PROPS, indent=2)}\")\n",
    "\n",
    "# Identifier fields serve as primary keys\n",
    "print(\n",
    "    \"Identifier fields:\",\n",
    "    {fid: HYDROLOGY_SCHEMA.find_field(fid).name for fid in HYDROLOGY_SCHEMA.identifier_field_ids},\n",
    ")\n",
    "\n",
    "HYDRAULICS_PARTITION_SPEC = auto_partition_spec(HYDRAULICS_SCHEMA, [\"realization_id\", \"model_id\", \"run_version\"])\n",
    "print(f\"Partition Spec: {HYDRAULICS_PARTITION_SPEC}\")\n",
    "\n",
    "HYDRAULICS_TABLE = \"hydraulics\"\n",
    "PG_HYDRAULICS_TABLE_NAME = f\"{TABLE_NAME_SPACE}.{HYDRAULICS_TABLE}\"\n",
    "S3_DATA_LOCATION = f\"{CATALOG_ROOT}/{TABLE_NAME_SPACE}/{HYDRAULICS_TABLE}\"\n",
    "\n",
    "\n",
    "\n",
    "if catalog.table_exists(PG_HYDRAULICS_TABLE_NAME):\n",
    "    print(f\"Table `{PG_HYDRAULICS_TABLE_NAME}` already exists.\")\n",
    "else:\n",
    "    # Create the Iceberg table\n",
    "    catalog.create_table(\n",
    "        identifier=PG_HYDRAULICS_TABLE_NAME,\n",
    "        schema=HYDRAULICS_SCHEMA,\n",
    "        location=S3_DATA_LOCATION,\n",
    "        properties=HYDRAULICS_PROPS,\n",
    "        partition_spec=HYDRAULICS_PARTITION_SPEC\n",
    "    )\n",
    "    print(f\"Table `{PG_HYDRAULICS_TABLE_NAME}` has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c3729",
   "metadata": {},
   "source": [
    "#### Both tables have now been created and are ready for data.\n",
    "\n",
    "![](imgs/nimtable-cat-2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lakehouse-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
