{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfb9588",
   "metadata": {},
   "source": [
    "### Initialize Catalog\n",
    "\n",
    " - Connect to catalog (postgres) and warehouse (s3 - data & metadata layer)\n",
    " - Create Namespace and Table\n",
    " - Load sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8f59cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connected to Iceberg catalog: `trinity`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing namespaces: []\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from connection import connect_to_catalog\n",
    "catalog = connect_to_catalog()\n",
    "\n",
    "name_spaces = [ns[0] for ns in catalog.list_namespaces()]\n",
    "print(f\"Existing namespaces: {name_spaces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec74fe5",
   "metadata": {},
   "source": [
    "### What did that do?\n",
    "\n",
    "Connecting to the catalog via the `connect_to_catalog` function:\n",
    "\n",
    "1. Establishes a connection with the jdbc catalog (i.e. postgres) and data/metadata warehouse (i.e. s3)\n",
    "\n",
    "2. With the connection established, the following tables are automatically added to the `public` schema if they do not exist:\n",
    "\n",
    "![](imgs/jdbc-cat.png)\n",
    "\n",
    "Note that we are using the postgres database from the container `pgduckdb/pgduckdb:17-main`. This image comes with the [pg_duckdb](https://github.com/duckdb/pg_duckdb) extension and schema pre-installed.\n",
    "\n",
    "---\n",
    "\n",
    "#### Now we can read in a schema to prepare for creating a table\n",
    "\n",
    "To do that we first need a table schema and optional partition specification. Then can set the target and create table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg Table Schema: table {\n",
      "  1: sim_time: required timestamp (Simulation timestamp from HEC-HMS model [UTC])\n",
      "  2: realization_id: required int (Unique identifier for each model realization)\n",
      "  3: model_id: required string (Identifier for the HEC-HMS model)\n",
      "  4: site_id: required string (Identifier for the measurement site)\n",
      "  5: event_id: required int (Identifier for the simulated event)\n",
      "  6: run_version: required string (Version of the model run)\n",
      "  7: flow: optional double (Discharge at the site [cfs])\n",
      "  8: base_flow: optional double (Baseflow at the site [cfs])\n",
      "}\n",
      "Iceberg Table Properties: {\n",
      "  \"hydrology.schema.version\": \"1.0.0\",\n",
      "  \"hydrology.description\": \"HEC-HMS simulation outputs\",\n",
      "  \"hydrology.units.convention\": \"English\",\n",
      "  \"hydrology.model.version\": \"4.13\",\n",
      "  \"hydrology.time.step\": \"15min\",\n",
      "  \"hydrology.time.timezone\": \"UTC\",\n",
      "  \"hydrology.stac.catalog\": \"s3://trinity-pilot/stac/hydrology/catalog.json\",\n",
      "  \"write.hive-style-partitioning\": \"true\",\n",
      "  \"write.target-file-size-bytes\": \"536870912\",\n",
      "  \"field.sim_time.unit\": \"UTC\",\n",
      "  \"field.sim_time.description\": \"Simulation timestamp from HEC-HMS model\",\n",
      "  \"field.realization_id.description\": \"Unique identifier for each model realization\",\n",
      "  \"field.model_id.description\": \"Identifier for the HEC-HMS model\",\n",
      "  \"field.site_id.description\": \"Identifier for the measurement site\",\n",
      "  \"field.event_id.description\": \"Identifier for the simulated event\",\n",
      "  \"field.run_version.description\": \"Version of the model run\",\n",
      "  \"field.flow.unit\": \"cfs\",\n",
      "  \"field.flow.description\": \"Discharge at the site\",\n",
      "  \"field.base_flow.unit\": \"cfs\",\n",
      "  \"field.base_flow.description\": \"Baseflow at the site\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from table_utils import load_iceberg_schema_and_properties\n",
    "import json\n",
    "\n",
    "# Load / Preview schema\n",
    "hms_schema_json = \"schemas/hydrology.json\"\n",
    "\n",
    "HYDROLOGY_SCHEMA, HYDROLOGY_PROPS = load_iceberg_schema_and_properties(hms_schema_json)\n",
    "\n",
    "print(f\"Iceberg Table Schema: {HYDROLOGY_SCHEMA}\")\n",
    "print(f\"Iceberg Table Properties: {json.dumps(HYDROLOGY_PROPS, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56280f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Spec: [\n",
      "  100: realization_id: identity(2)\n",
      "  101: model_id: identity(3)\n",
      "  102: run_version: identity(6)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# This dataset will grow to many billions of rows, so partitioning is important.\n",
    "# Note: We can use predicate pushdown later on unpartitioned columns\n",
    "from table_utils import auto_partition_spec\n",
    "HYDROLOGY_PARTITION_SPEC = auto_partition_spec(HYDROLOGY_SCHEMA, [\"realization_id\", \"model_id\", \"run_version\"])\n",
    "print(f\"Partition Spec: {HYDROLOGY_PARTITION_SPEC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf019991",
   "metadata": {},
   "source": [
    "\n",
    "##### Now we can add a `namespace` for create an iceberg `table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a56a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Root: s3://trinity-pilot/warehouse\n",
      "Namespace already exists: conformance\n"
     ]
    }
   ],
   "source": [
    "# Set target: Load existing variables from iceberg/connection.py:\n",
    "# Create table namespace\n",
    "from connection import CATALOG_ROOT\n",
    "print(f\"Catalog Root: {CATALOG_ROOT}\")  \n",
    "\n",
    "# Assign table-specific variables\n",
    "TABLE_NAME_SPACE = \"conformance\"\n",
    "\n",
    "name_spaces = [ns[0] for ns in catalog.list_namespaces()]\n",
    "if TABLE_NAME_SPACE not in name_spaces:\n",
    "    catalog.create_namespace(TABLE_NAME_SPACE)\n",
    "    print(f\"Created namespace: {TABLE_NAME_SPACE}\")\n",
    "else:\n",
    "    print(f\"Namespace already exists: {TABLE_NAME_SPACE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b28e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n",
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n",
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table `conformance.hydrology` has been created.\n"
     ]
    }
   ],
   "source": [
    "HYDROLOGY_TABLE = \"hydrology\"\n",
    "PG_HYDROLOGY_TABLE_NAME = f\"{TABLE_NAME_SPACE}.{HYDROLOGY_TABLE}\"\n",
    "S3_DATA_LOCATION = f\"{CATALOG_ROOT}/{TABLE_NAME_SPACE}/{HYDROLOGY_TABLE}\"\n",
    "\n",
    "\n",
    "\n",
    "if catalog.table_exists(PG_HYDROLOGY_TABLE_NAME):\n",
    "    print(f\"Table `{PG_HYDROLOGY_TABLE_NAME}` already exists.\")\n",
    "else:\n",
    "    # Create the Iceberg table\n",
    "    catalog.create_table(\n",
    "        identifier=PG_HYDROLOGY_TABLE_NAME,\n",
    "        schema=HYDROLOGY_SCHEMA,\n",
    "        location=S3_DATA_LOCATION,\n",
    "        properties=HYDROLOGY_PROPS,\n",
    "        partition_spec=HYDROLOGY_PARTITION_SPEC\n",
    "    )\n",
    "    print(f\"Table `{PG_HYDROLOGY_TABLE_NAME}` has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba33538",
   "metadata": {},
   "source": [
    "### Connect the newly created catalog to nimtable\n",
    "\n",
    "The Nimtable utility is a handy reference utility for viewing table data.\n",
    "\n",
    "Enter the information in the `Create Catalog` button available at http://localhost/3000\n",
    "\n",
    "![](imgs/new-nimtable-cat.png)\n",
    "\n",
    "Then view the table:\n",
    "\n",
    "![](imgs/cat-page-1.png)\n",
    "\n",
    "![](imgs/cat-page-2.png)\n",
    "\n",
    "![](imgs/cat-page-3.png)\n",
    "\n",
    "*NOTE* See schemas/hydrology for addtional properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be482228",
   "metadata": {},
   "source": [
    "#### Updates acheived:\n",
    "\n",
    "1. The postgres catalog no points to the newly create table in the conformance namespace:\n",
    "\n",
    "![](imgs/pg-iceberg-table-1.png)\n",
    "\n",
    "2. A newly created metadata file has been written to the s3 catalog + namespace location:\n",
    "\n",
    "![](imgs/bucket-view-1.png)\n",
    "\n",
    "*NOTE* There are no data files yet. \n",
    "\n",
    "---\n",
    "\n",
    "#### Finally, lets add a Hydraulics table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b81e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg Table Schema: table {\n",
      "  1: sim_time: required timestamp (Simulation timestamp from HEC-RAS model [UTC])\n",
      "  2: realization_id: required int (Unique identifier for each model realization)\n",
      "  3: model_id: required string (Identifier for the HEC-RAS model)\n",
      "  4: site_id: required string (Identifier for the measurement site)\n",
      "  5: event_id: required int (Identifier for the simulated event)\n",
      "  6: run_version: required string (Version of the model run)\n",
      "  7: flow: optional double (Discharge at the site [cfs])\n",
      "  8: stage: optional double (Stage at the site [ft])\n",
      "}\n",
      "Iceberg Table Properties: {\n",
      "  \"hydraulics.schema.version\": \"1.0.0\",\n",
      "  \"hydraulics.description\": \"HEC-RAS simulation outputs\",\n",
      "  \"hydraulics.units.convention\": \"English\",\n",
      "  \"hydraulic.model.version\": \"6.6\",\n",
      "  \"hydraulics.stac.catalog\": \"s3://trinity-pilot/stac/hydraulics/catalog.json\",\n",
      "  \"write.hive-style-partitioning\": \"true\",\n",
      "  \"write.target-file-size-bytes\": \"536870912\",\n",
      "  \"field.sim_time.unit\": \"UTC\",\n",
      "  \"field.sim_time.description\": \"Simulation timestamp from HEC-RAS model\",\n",
      "  \"field.realization_id.description\": \"Unique identifier for each model realization\",\n",
      "  \"field.model_id.description\": \"Identifier for the HEC-RAS model\",\n",
      "  \"field.site_id.description\": \"Identifier for the measurement site\",\n",
      "  \"field.event_id.description\": \"Identifier for the simulated event\",\n",
      "  \"field.run_version.description\": \"Version of the model run\",\n",
      "  \"field.flow.unit\": \"cfs\",\n",
      "  \"field.flow.description\": \"Discharge at the site\",\n",
      "  \"field.stage.unit\": \"ft\",\n",
      "  \"field.stage.description\": \"Stage at the site\"\n",
      "}\n",
      "Partition Spec: [\n",
      "  100: realization_id: identity(2)\n",
      "  101: model_id: identity(3)\n",
      "  102: run_version: identity(6)\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n",
      "INFO:pyiceberg.io:Loaded FileIO: pyiceberg.io.pyarrow.PyArrowFileIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table `conformance.hydraulics` has been created.\n"
     ]
    }
   ],
   "source": [
    "#### Hydraulics Table\n",
    "\n",
    "# Load / Preview schema\n",
    "ras_schema_json = \"schemas/hydraulics.json\"\n",
    "\n",
    "HYDRAULICS_SCHEMA, HYDRAULICS_PROPS = load_iceberg_schema_and_properties(ras_schema_json)\n",
    "\n",
    "print(f\"Iceberg Table Schema: {HYDRAULICS_SCHEMA}\")\n",
    "print(f\"Iceberg Table Properties: {json.dumps(HYDRAULICS_PROPS, indent=2)}\")\n",
    "\n",
    "HYDRAULICS_PARTITION_SPEC = auto_partition_spec(HYDRAULICS_SCHEMA, [\"realization_id\", \"model_id\", \"run_version\"])\n",
    "print(f\"Partition Spec: {HYDRAULICS_PARTITION_SPEC}\")\n",
    "\n",
    "HYDRAULICS_TABLE = \"hydraulics\"\n",
    "PG_HYDRAULICS_TABLE_NAME = f\"{TABLE_NAME_SPACE}.{HYDRAULICS_TABLE}\"\n",
    "S3_DATA_LOCATION = f\"{CATALOG_ROOT}/{TABLE_NAME_SPACE}/{HYDRAULICS_TABLE}\"\n",
    "\n",
    "\n",
    "\n",
    "if catalog.table_exists(PG_HYDRAULICS_TABLE_NAME):\n",
    "    print(f\"Table `{PG_HYDRAULICS_TABLE_NAME}` already exists.\")\n",
    "else:\n",
    "    # Create the Iceberg table\n",
    "    catalog.create_table(\n",
    "        identifier=PG_HYDRAULICS_TABLE_NAME,\n",
    "        schema=HYDRAULICS_SCHEMA,\n",
    "        location=S3_DATA_LOCATION,\n",
    "        properties=HYDRAULICS_PROPS,\n",
    "        partition_spec=HYDRAULICS_PARTITION_SPEC\n",
    "    )\n",
    "    print(f\"Table `{PG_HYDRAULICS_TABLE_NAME}` has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c3729",
   "metadata": {},
   "source": [
    "#### Both tables have now been created and are ready for data.\n",
    "\n",
    "![](imgs/nimtable-cat-2.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
